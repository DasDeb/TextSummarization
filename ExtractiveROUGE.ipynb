{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFull=pd.read_csv(\"Reviews.csv\")\n",
    "dataFull.drop(['Id','ProductId','UserId','ProfileName','HelpfulnessNumerator','HelpfulnessDenominator','Score','Time'],axis=1,inplace=True)\n",
    "#dataFull.drop_duplicates(subset=['Text'],inplace=True)#dropping duplicates\n",
    "dataFull.dropna(axis=0,inplace=True)#dropping na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568427, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFull.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>568449</th>\n",
       "      <td>Will not do without</td>\n",
       "      <td>Great for sesame chicken..this is a good if no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568450</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>I'm disappointed with the flavor. The chocolat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568451</th>\n",
       "      <td>Perfect for our maltipoo</td>\n",
       "      <td>These stars are small, so you can give 10-15 o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568452</th>\n",
       "      <td>Favorite Training and reward treat</td>\n",
       "      <td>These are the BEST treats for training and rew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568453</th>\n",
       "      <td>Great Honey</td>\n",
       "      <td>I am very satisfied ,product is as advertised,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Summary  \\\n",
       "568449                 Will not do without   \n",
       "568450                        disappointed   \n",
       "568451            Perfect for our maltipoo   \n",
       "568452  Favorite Training and reward treat   \n",
       "568453                         Great Honey   \n",
       "\n",
       "                                                     Text  \n",
       "568449  Great for sesame chicken..this is a good if no...  \n",
       "568450  I'm disappointed with the flavor. The chocolat...  \n",
       "568451  These stars are small, so you can give 10-15 o...  \n",
       "568452  These are the BEST treats for training and rew...  \n",
       "568453  I am very satisfied ,product is as advertised,...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFull.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=dataFull.tail(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Until my wife and I started a weight loss effort earlier this summer, we hadn't eaten all that much Progresso soup. Because of their variety of flavors and low calories, we decided to give them a try, and I'm very glad we did.<br /><br />This soup is only 60 calories per serving, but has great flavor, as do all the Progresso soups we've tried. Many lo-cal soups are just blah tasting, but Progresso puts a nice blend of spices into everything we've tried, and this soup is no exception. I'm quite impressed and heartily recommend this variety to dieters who'd like a low calorie meal that they can enjoy the taste of!\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1=df[\"Text\"][468451]\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Because of their variety of flavors and low calories, we decided to give them a try, and I'm very glad we did.<br /><br />This soup is only 60 calories per serving, but has great flavor, as do all the Progresso soups we've tried.\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from heapq import nlargest\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def summarize(text,n):\n",
    "    sents=sent_tokenize(text)\n",
    "    \n",
    "    for i in range(len(sents)):\n",
    "        row=sents[i]\n",
    "        row = row.replace(\"\\n\", \"\")\n",
    "        row = row.replace(\"\\t\", \"\")\n",
    "        row = row.replace(\"-\", \"\")\n",
    "        sents[i] = row \n",
    "    \n",
    "           \n",
    "    assert n<=len(sents)\n",
    "    word_sent=word_tokenize(text.lower())\n",
    "    freq=FreqDist(word_sent)\n",
    "    nlargest(20,freq,key=freq.get)\n",
    "    customStopwords=set(stopwords.words('english')+list(punctuation)+list(\"\\n\"))\n",
    "    word_sent=[word for word in word_sent if word not in customStopwords]\n",
    "    \n",
    "    print(len(sents))\n",
    "    \n",
    "    ranking=defaultdict(int)\n",
    "    for i,sent in enumerate(sents):\n",
    "        for w in word_tokenize(sent.lower()):\n",
    "            if w in freq:\n",
    "                ranking[i]+=freq[w]\n",
    "    sents_idx=nlargest(n,ranking,key=ranking.get)\n",
    "    return [sents[j] for j in sorted(sents_idx)]\n",
    "    \n",
    "\n",
    "summarize(t1,1)\n",
    "\n",
    "#Analyze frequency of words and see effect of rare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexranks Summary:\n",
      "Until my wife and I started a weight loss effort earlier this summer, we hadn't eaten all that much Progresso soup.\n",
      "\n",
      "\n",
      "LSA Summary:\n",
      "Until my wife and I started a weight loss effort earlier this summer, we hadn't eaten all that much Progresso soup.\n",
      "\n",
      "\n",
      "Luhn's Summary:\n",
      "Because of their variety of flavors and low calories, we decided to give them a try, and I'm very glad we did.<br /><br />This soup is only 60 calories per serving, but has great flavor, as do all the Progresso soups we've tried.\n"
     ]
    }
   ],
   "source": [
    "from sumy.summarizers import luhn\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from gensim.summarization import summarize\n",
    "from sumy.summarizers.luhn import LuhnSummarizer \n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
    "\n",
    "LANGUAGE = \"english\"\n",
    "SENTENCES_COUNT = 1\n",
    "\n",
    "#text=t1\n",
    "\n",
    "#parser = PlaintextParser.from_string((text), Tokenizer(LANGUAGE))\n",
    "stemmer = Stemmer(LANGUAGE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def lexrank_summarizer(text):\n",
    "    parser = PlaintextParser.from_string((text), Tokenizer(LANGUAGE))\n",
    "    summarizer_LexRank = LexRankSummarizer(stemmer)\n",
    "    summarizer_LexRank.stop_words = get_stop_words(LANGUAGE)\n",
    "    output=''\n",
    "    for sentence in summarizer_LexRank(parser.document, SENTENCES_COUNT):\n",
    "        output+=str(sentence)\n",
    "    return output\n",
    "        \n",
    "        \n",
    "def lsa_summarizer(text):\n",
    "    parser = PlaintextParser.from_string((text), Tokenizer(LANGUAGE))\n",
    "    summarizer_lsa = Summarizer(stemmer)\n",
    "    summarizer_lsa.stop_words = get_stop_words(LANGUAGE)\n",
    "    ab=\"\"\n",
    "    for sentence in summarizer_lsa(parser.document, SENTENCES_COUNT):\n",
    "        #print (sentence)\n",
    "        #print(type(sentence))\n",
    "        ab+=str(sentence)\n",
    "    return ab\n",
    "   # return sentence\n",
    "def luhn_summarizer(text):\n",
    "    parser = PlaintextParser.from_string((text), Tokenizer(LANGUAGE))\n",
    "    summarizer_luhn = LuhnSummarizer(stemmer)\n",
    "    summarizer_luhn.stop_words = get_stop_words(LANGUAGE)\n",
    "    abc=''\n",
    "    for sentence in summarizer_luhn(parser.document, SENTENCES_COUNT):\n",
    "        abc+=str(sentence)\n",
    "    return abc\n",
    "        \n",
    "def gensim_summarizer():\n",
    "    print(summarize(text))\n",
    "\n",
    "print(\"Lexranks Summary:\")\n",
    "print(lexrank_summarizer(t1))\n",
    "print(\"\\n\")\n",
    "print(\"LSA Summary:\")\n",
    "print(lsa_summarizer(t1))\n",
    "print(\"\\n\")\n",
    "print(\"Luhn's Summary:\")\n",
    "print(luhn_summarizer(t1))\n",
    "#print(\"========\")\n",
    "#gensim_summarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "492\n"
     ]
    }
   ],
   "source": [
    "r1=0.0\n",
    "r2=0.0\n",
    "rl=0.0\n",
    "ce=0\n",
    "rouge=Rouge()\n",
    "for i in range(468450,568454):\n",
    "    #print(df[\"Summary\"][i])\n",
    "    try:\n",
    "        sc=rouge.get_scores(lexrank_summarizer(df[\"Text\"][i]),df[\"Summary\"][i])\n",
    "    except:\n",
    "        ce+=1\n",
    "        continue\n",
    "    r1+=sc[0]['rouge-1']['f']\n",
    "    r2+=sc[0]['rouge-2']['f']\n",
    "    rl+=sc[0]['rouge-l']['f']\n",
    "print(ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06599310420831273\n",
      "0.01720169629091797\n",
      "0.04830501133640649\n"
     ]
    }
   ],
   "source": [
    "print(r1/99508)\n",
    "print(r2/99508)\n",
    "print(rl/99508)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishan\\Anaconda3\\lib\\site-packages\\sumy\\summarizers\\lsa.py:76: UserWarning: Number of words (3) is lower than number of sentences (4). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "C:\\Users\\ishan\\Anaconda3\\lib\\site-packages\\sumy\\summarizers\\lsa.py:76: UserWarning: Number of words (4) is lower than number of sentences (5). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "C:\\Users\\ishan\\Anaconda3\\lib\\site-packages\\sumy\\summarizers\\lsa.py:76: UserWarning: Number of words (6) is lower than number of sentences (7). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "C:\\Users\\ishan\\Anaconda3\\lib\\site-packages\\sumy\\summarizers\\lsa.py:76: UserWarning: Number of words (5) is lower than number of sentences (8). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "C:\\Users\\ishan\\Anaconda3\\lib\\site-packages\\sumy\\summarizers\\lsa.py:76: UserWarning: Number of words (5) is lower than number of sentences (7). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "C:\\Users\\ishan\\Anaconda3\\lib\\site-packages\\sumy\\summarizers\\lsa.py:76: UserWarning: Number of words (2) is lower than number of sentences (3). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "C:\\Users\\ishan\\Anaconda3\\lib\\site-packages\\sumy\\summarizers\\lsa.py:76: UserWarning: Number of words (31) is lower than number of sentences (33). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "C:\\Users\\ishan\\Anaconda3\\lib\\site-packages\\sumy\\summarizers\\lsa.py:76: UserWarning: Number of words (32) is lower than number of sentences (94). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "C:\\Users\\ishan\\Anaconda3\\lib\\site-packages\\sumy\\summarizers\\lsa.py:76: UserWarning: Number of words (217) is lower than number of sentences (346). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "C:\\Users\\ishan\\Anaconda3\\lib\\site-packages\\sumy\\summarizers\\lsa.py:76: UserWarning: Number of words (2) is lower than number of sentences (4). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "C:\\Users\\ishan\\Anaconda3\\lib\\site-packages\\sumy\\summarizers\\lsa.py:76: UserWarning: Number of words (5) is lower than number of sentences (6). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "C:\\Users\\ishan\\Anaconda3\\lib\\site-packages\\sumy\\summarizers\\lsa.py:76: UserWarning: Number of words (3) is lower than number of sentences (8). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "493\n"
     ]
    }
   ],
   "source": [
    "rr1=0.0\n",
    "rr2=0.0\n",
    "rrl=0.0\n",
    "c=0\n",
    "rouge2=Rouge()\n",
    "for i in range(468450,568454):\n",
    "    #print(df[\"Summary\"][i])\n",
    "    try:\n",
    "        sco=rouge2.get_scores(lsa_summarizer(df[\"Text\"][i]),df[\"Summary\"][i])\n",
    "    except:\n",
    "        c+=1\n",
    "        continue\n",
    "    rr1+=sco[0]['rouge-1']['f']\n",
    "    rr2+=sco[0]['rouge-2']['f']\n",
    "    rrl+=sco[0]['rouge-l']['f']\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04974007379017464\n",
      "0.010007397639722198\n",
      "0.03342907689476802\n"
     ]
    }
   ],
   "source": [
    "print(rr1/99507)\n",
    "print(rr2/99507)\n",
    "print(rrl/99507)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rrr1=0.0\n",
    "rrr2=0.0\n",
    "rrrl=0.0\n",
    "rouge3=Rouge()\n",
    "count=0\n",
    "for i in range(468450,568454):\n",
    "    #print(df[\"Summary\"][i])\n",
    "    try:\n",
    "        scor=rouge3.get_scores(luhn_summarizer(df[\"Text\"][i]),df[\"Summary\"][i])\n",
    "    except:\n",
    "        count+=1\n",
    "        continue\n",
    "    rrr1+=scor[0]['rouge-1']['f']\n",
    "    rrr2+=scor[0]['rouge-2']['f']\n",
    "    rrrl+=scor[0]['rouge-l']['f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494\n"
     ]
    }
   ],
   "source": [
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06166717654986019\n",
      "0.015335679932927442\n",
      "0.04407498644017556\n"
     ]
    }
   ],
   "source": [
    "print(rrr1/99506)\n",
    "print(rrr2/99506)\n",
    "print(rrrl/99506)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34229\n"
     ]
    }
   ],
   "source": [
    "o1=0.0\n",
    "o2=0.0\n",
    "ol=0.0\n",
    "cou=0\n",
    "rouge4=Rouge()\n",
    "for i in range(468450,568454):\n",
    "    #print(i)\n",
    "    tot=\"\"\n",
    "    try:\n",
    "        for s in summarize(df[\"Text\"][i],1):\n",
    "            tot+=str(s)\n",
    "    #print(tot)\n",
    "        score=rouge3.get_scores(tot,df[\"Summary\"][i])\n",
    "    except:\n",
    "        cou+=1\n",
    "        continue\n",
    "    o1+=score[0]['rouge-1']['f']\n",
    "    o2+=score[0]['rouge-2']['f']\n",
    "    ol+=score[0]['rouge-l']['f']\n",
    "print(cou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.028903905626081347\n",
      "0.0057598653759092355\n",
      "0.015804811527395057\n"
     ]
    }
   ],
   "source": [
    "print(o1/99988)\n",
    "print(o2/99988)\n",
    "print(ol/99988)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17591572932902222\n"
     ]
    }
   ],
   "source": [
    "print(100000/568454)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
